{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-M8jTjKiYBo"
      },
      "source": [
        "# Лабораторная работа №1. Регрессия\n",
        "\n",
        "**Тема:** *Сравнение линейной регрессии, случайного леса (Random Forest Regressor), градиентного бустинга (XGBoost) на задаче регрессии.*\n",
        "\n",
        "**Цели:**\n",
        "\n",
        "- *Научиться строить, оптимизировать и оценивать регрессионные модели.*\n",
        "- *Понять, как интерпретировать важность признаков (feature importance).*\n",
        "- *Понять, как увеличивать кол-во признаков (feature tuning).*\n",
        "- *Исследовать влияние преобразований признаков (фичей), регуляризации и гиперпараметров.*\n",
        "\n",
        "**Условия:**\n",
        "\n",
        "- *numpy, pandas, scikit-learn, XGBoost*\n",
        "- *выбрать датасет для 1-ой и 2-ой лабораторной работы*\n",
        "\n",
        "**Пункты:**\n",
        "\n",
        "1. Подготовка данных + Feature Tuning\n",
        "2. Создание метрик\n",
        "3. Базовая модель линейной регрессии\n",
        "4. Улучшение линейной регрессии\n",
        "5. Случайный лес регрессор\n",
        "6. Градиентный бустинг (XGBoost)\n",
        "7. Feature Importance\n",
        "8. Написание своих реализаций (классы)\n",
        "9. Подведение итогов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah66Ra94kcXI"
      },
      "source": [
        "## О команде (ЗАПОЛНИТЬ СВОИМИ ДАННЫМИ)\n",
        "\n",
        "| Фамилия И.О.        | Группа | Роль в команде | Что делал                    |\n",
        "|---------------------|--------|----------------|------------------------------|\n",
        "| Солодова С. М.         | 309    |        |                       |\n",
        "| Штыхно И.          | 309    |        |                       |\n",
        "| Мазепа И.         | 309    |        |                       |\n",
        "\n",
        " Датасет: [student habits vs academic perfomance](https://www.kaggle.com/datasets/jayaantanaath/student-habits-vs-academic-performance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDbayIFnmbDQ"
      },
      "source": [
        "## Описание датасета:\n",
        "\n",
        "Датасет Student Habits & Performance представляет собой комплексное исследование факторов, влияющих на академическую успеваемость студентов. Он содержит данные о 1000 студентах и включает 16 различных характеристик, охватывающих демографические показатели, учебные привычки, образ жизни и социально-экономические факторы. Целевой переменной является exam_score — экзаменационные баллы студентов, которые варьируются от 18.4 до 100.0 баллов со средним значением 69.6 балла.\n",
        "\n",
        "Датасет структурирован по четырем основным блокам признаков: демографические данные (возраст, пол), академические факторы (часы учебы в день, процент посещаемости, уровень образования родителей), характеристики образа жизни (время в социальных сетях, просмотр Netflix, часы сна, качество питания, частота упражнений) и факторы окружающей среды (подработка, качество интернета, оценка психического здоровья, участие во внеучебной деятельности). Это делает датасет особенно ценным для изучения многофакторного влияния на успеваемость студентов и построения предиктивных моделей в области образовательной аналитики. Одна из проблем качества данных — пропуски в признаке уровня образования родителей, что требует предварительной обработки перед анализом.\n",
        "\n",
        "---\n",
        "\n",
        "### Таблица признаков\n",
        "\n",
        "| № | Признак                      | Тип данных | Описание                                                                 |\n",
        "|---|------------------------------|------------|--------------------------------------------------------------------------|\n",
        "| 1 | student_id                   | object     | Уникальный идентификатор студента (S1000-S1999)                         |\n",
        "| 2 | age                          | int        | Возраст студента (17-24 лет)                                            |\n",
        "| 3 | gender                       | object     | Пол студента (Female, Male, Other)                                      |\n",
        "| 4 | study_hours_per_day          | float      | Количество часов учебы в день (0.0-8.3)                                 |\n",
        "| 5 | social_media_hours           | float      | Часы в социальных сетях в день (0.0-7.2)                                |\n",
        "| 6 | netflix_hours                | float      | Часы просмотра Netflix в день (0.0-5.4)                                 |\n",
        "| 7 | part_time_job                | object     | Наличие подработки (No, Yes)                                            |\n",
        "| 8 | attendance_percentage        | float      | Процент посещаемости занятий (56.0-100.0%)                              |\n",
        "| 9 | sleep_hours                  | float      | Количество часов сна в сутки (3.2-10.0)                                 |\n",
        "| 10| diet_quality                 | object     | Качество питания (Fair, Good, Poor)                                     |\n",
        "| 11| exercise_frequency           | int        | Частота физических упражнений в неделю (0-6)                            |\n",
        "| 12| parental_education_level     | object     | Уровень образования родителей (Master, High School, Bachelor) - 9.1% пропусков |\n",
        "| 13| internet_quality             | object     | Качество интернет-соединения (Average, Poor, Good)                      |\n",
        "| 14| mental_health_rating         | int        | Самооценка психического здоровья (шкала 1-10)                           |\n",
        "| 15| extracurricular_participation| object     | Участие во внеучебной деятельности (Yes, No)                            |\n",
        "| 16| exam_score                   | float      | **ЦЕЛЕВАЯ ПЕРЕМЕННАЯ** - Экзаменационный балл (18.4-100.0, среднее: 69.6) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyNH9NF-g9J"
      },
      "source": [
        "## 0. Глобальная настройка проекта\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaQorah1-lBy"
      },
      "outputs": [],
      "source": [
        "RND_SEED = 21\n",
        "USE_AUTO_POLY = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMbcyFTXnqmH"
      },
      "source": [
        "## 1. Подготовка данных + Feature Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fSSuTDjnuXu"
      },
      "source": [
        "### 1.1. Загрузка датасета"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEXdpJjn1uo"
      },
      "source": [
        "Подключим `Google Drive` и загрузим наш датасет используя `Pandas.DataFrame`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pKVsiCAo7Vm"
      },
      "source": [
        "Подключение к гугл диску"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKBmKQsHo3cq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "drive_path = Path('/content/drive')\n",
        "drive.mount(str(drive_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJC5TWYAo-i6"
      },
      "source": [
        "Загрузка датасета `pd.read_csv(path_to_dataset: str)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miSqROM7iNdP"
      },
      "outputs": [],
      "source": [
        "file_path = drive_path / 'MyDrive' / 'student_habits_performance.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a83URAnfpevD"
      },
      "source": [
        "Получение статистки по датасету, необходимую для анализа. Подумать что можно сделать и какие выводы нужны."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YuWJYTWp1Ig"
      },
      "source": [
        "Посмотрим первые три строки датасета `df.head(n: int)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heDdlkyHpi00"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APmlq1NSzp4d"
      },
      "source": [
        "У нас тут имеется ненужный атрибут `student_id`. Можем его удалить."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIfrJQg4zzSw"
      },
      "outputs": [],
      "source": [
        "if 'student_id' in df.columns:\n",
        "    df = df.drop(columns=['student_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcG5JBi10jkC"
      },
      "source": [
        "### 1.2. Сбор основной информации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnamOveQp-RQ"
      },
      "source": [
        "Получение общей информации `df.info()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfHqpt7kp99V"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITnysOsut2jI"
      },
      "outputs": [],
      "source": [
        "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "def normalize_series(s: pd.Series) -> pd.Series:\n",
        "    return (\n",
        "        s.astype(str)\n",
        "         .str.strip()\n",
        "         .str.replace('\\u00A0', ' ', regex=False)   # NBSP -> space\n",
        "         .str.replace(r'\\s+', ' ', regex=True)      # collapse spaces\n",
        "         .str.title()                               # простая капитализация\n",
        "    )\n",
        "\n",
        "print('Уникальные значения (нормализованные):')\n",
        "normalized_uniques = {}\n",
        "for col in cat_cols:\n",
        "    vals = normalize_series(df[col])\n",
        "    normalized_uniques[col] = sorted(vals.unique().tolist())\n",
        "    print(f'- {col}: {normalized_uniques[col]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw3TGggJqU65"
      },
      "source": [
        "Получение всей статистики `df.describe()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX6O1u3IqZcJ"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A23uJ4lNrrQs"
      },
      "source": [
        "**Вывод:**\n",
        "####**Общая характеристика данных:**\n",
        "* Выборка включает 1000 студентов с полными данными (нет пропусков)\n",
        "\n",
        "* Возраст студентов: 17-24 года (среднее 20.5 лет)\n",
        "\n",
        "* Экзаменационные баллы варьируются от 18.4 до 100.0 (среднее 69.6)\n",
        "\n",
        "####**Ключевые наблюдения:**\n",
        "\n",
        "**Учебные привычки:**\n",
        "* Посещаемость высокая (среднее 84.1%, минимум 56%)\n",
        "\n",
        "* Учеба: 3.55 часа/день в среднем, но разброс большой (0-8.3 часа)\n",
        "\n",
        "* 25% студентов учатся меньше 2.6 часов в день\n",
        "\n",
        "**Времяпрепровождение:**\n",
        "* Соцсети: 2.5 часа/день в среднем, некоторые до 7.2 часа\n",
        "\n",
        "* Netflix: 1.8 часа/день, относительно умеренное использование\n",
        "\n",
        "* Сон: 6.47 часа в среднем, но 25% студентов спят меньше 5.6 часа\n",
        "\n",
        "**Здоровье и активность:**\n",
        "* Физнагрузки: 3 раза/неделю в среднем, но 25% студентов почти не занимаются (≤1 раз)\n",
        "\n",
        "* Психическое здоровье: средняя оценка 5.44/10, значительный разброс (1-10)\n",
        "\n",
        "**Потенциальные проблемы:**\n",
        "* Недостаток сна у значительной части студентов\n",
        "\n",
        "* Низкая физическая активность у 25% выборки\n",
        "\n",
        "* Высокий разброс учебного времени указывает на неравномерную нагрузку"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLrErhBLq3tu"
      },
      "source": [
        "Проверка на пропуски данных `df.isnull().sum()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozlugi5prAjx"
      },
      "outputs": [],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE88PfWqZF0N"
      },
      "source": [
        "В выводе видно, что в столбце \"parental_education_level\" есть пропущенные значения.\n",
        "\n",
        "Выведим уникальные значения этой колонки, чтобы далее принять решения как с ним работать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3SfTnlM0C_L"
      },
      "outputs": [],
      "source": [
        "print((df['parental_education_level']).unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKLxQskerKbm"
      },
      "source": [
        "**Вывод:** в датасетe пропуски. Заменим пустые значения модой.\n",
        "\n",
        "**Мода** - самое часто встречающееся значение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69aSGQrC8rCZ"
      },
      "outputs": [],
      "source": [
        "df_processed = df.copy()\n",
        "education_mode = df_processed['parental_education_level'].mode()[0]\n",
        "df_processed['parental_education_level'] = df_processed['parental_education_level'].fillna(education_mode)\n",
        "print(df_processed.isnull().sum())\n",
        "df_processed.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf-hzWQssygw"
      },
      "source": [
        "Посмотрим на распределение целевой переменной"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLHJNLxXs-84"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "n, bins, patches = plt.hist(df['exam_score'], bins=25, alpha=0.7,\n",
        "                           color='lightblue', edgecolor='navy', linewidth=0.8)\n",
        "\n",
        "mean_score = df['exam_score'].mean()\n",
        "median_score = df['exam_score'].median()\n",
        "\n",
        "plt.axvline(mean_score, color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Среднее: {mean_score:.1f}')\n",
        "plt.axvline(median_score, color='green', linestyle='-', linewidth=2,\n",
        "           label=f'Медиана: {median_score:.1f}')\n",
        "\n",
        "plt.title('Распределение экзаменационных баллов студентов', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Экзаменационный балл', fontsize=12)\n",
        "plt.ylabel('Количество студентов', fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "stats_text = f'n = {len(df)}\\nСтд. откл. = {df[\"exam_score\"].std():.1f}'\n",
        "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,\n",
        "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lao2F7BMcViE"
      },
      "source": [
        "**Вывод:**\n",
        "\n",
        "Распределение экзаменационных баллов близко к нормальному с небольшим смещением влево. Медиана (70.5) немного выше среднего (69.6), что указывает на наличие небольшой группы студентов с низкими результатами, которые снижают общее среднее значение.\n",
        "\n",
        "Большинство студентов показывают результаты в среднем диапазоне, при этом экстремально низкие баллы встречаются реже, чем высокие, что свидетельствует о достаточно успешной академической подготовке основной массы студентов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjuJGCzqHG7y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Один вертикальный box plot\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "ax.boxplot(\n",
        "    df['exam_score'],\n",
        "    patch_artist=True,\n",
        "    boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
        "    medianprops=dict(color='darkred', linewidth=2)\n",
        ")\n",
        "\n",
        "ax.set_title('Ящик с усами (вертикальный)')\n",
        "ax.set_ylabel('Экзаменационный балл')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_EocpPRF4Xp"
      },
      "source": [
        "Вывод: видим выбросы, малое количество баллов набрало значительно меньшее количество студентов. Значит, в моделях, чувствительных к выбросам, таких как линейная регрессия, нужно от них избавиться."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2FkyWKMSFC6"
      },
      "outputs": [],
      "source": [
        "# 3. КОДИРОВАНИЕ ПОРЯДКОВЫХ ПРИЗНАКОВ\n",
        "ordinal_mappings = {\n",
        "    'diet_quality': {'Poor': 1, 'Fair': 2, 'Good': 3},\n",
        "    'internet_quality': {'Poor': 1, 'Average': 2, 'Good': 3},\n",
        "    'parental_education_level': {'High School': 1, 'Bachelor': 2, 'Master': 3}\n",
        "}\n",
        "\n",
        "for col, mapping in ordinal_mappings.items():\n",
        "    df_processed[col] = df_processed[col].map(mapping)  # Используем df_processed\n",
        "\n",
        "# 4. ONE-HOT ENCODING ДЛЯ НОМИНАЛЬНЫХ ПРИЗНАКОВ\n",
        "nominal_cols = ['gender', 'part_time_job', 'extracurricular_participation']\n",
        "df_processed = pd.get_dummies(df_processed, columns=nominal_cols, prefix=nominal_cols, drop_first=True)\n",
        "\n",
        "#print(f\"Размерность матрицы: {correlation_matrix.shape}\")\n",
        "\n",
        "df_processed.info()\n",
        "df_processed.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUDynE9hvHey"
      },
      "source": [
        "**Вывод:** мы избавились от строковых значений в наших данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOHnIKStw7bk"
      },
      "source": [
        "Посмотрим на корреляцию данных.\n",
        "\n",
        "***Определение 1: Корреляция данных — это статистическая мера, показывающая, насколько и в каком направлении связаны между собой две переменные.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Oz9HIa_w0U7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def draw_corr_matrix(df):\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "    plt.title(\"Корреляционная матрица признаков\")\n",
        "    plt.show()\n",
        "\n",
        "draw_corr_matrix(df_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEGsfcJwYkUY"
      },
      "source": [
        "**Анализ корреляционной матрицы:**\n",
        "\n",
        "Корреляционный анализ выявил, что наиболее значимым предиктором экзаменационных баллов является количество часов учебы в день (r=0.82), демонстрируя сильную положительную связь. Умеренную положительную корреляцию с целевой переменной показывают оценка психического здоровья (r=0.31) и частота физических упражнений (r=0.15). Слабое негативное влияние на успеваемость оказывают время, проводимое в социальных сетях (r=-0.16) и просмотр Netflix (r=-0.16).\n",
        "\n",
        "Большинство остальных признаков демонстрируют слабые корреляции с целевой переменной (|r| < 0.3), что типично для социально-поведенческих данных. Анализ не выявил критических проблем мультиколлинеарности между предикторами, за исключением ожидаемых отрицательных корреляций между dummy-переменными пола. Полученные результаты подтверждают гипотезу о доминирующем влиянии академических привычек на учебные достижения студентов по сравнению с демографическими и lifestyle-факторами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P3UH-koec_m"
      },
      "source": [
        "**!!! ВАЖНО !!!**\n",
        "\n",
        "**ЕСЛИ МЫ СОБИРАЕМСЯ УЧИТЬ ЛИНЕЙНУЮ МОДЕЛЬ, И ДАННЫЕ КОРРЕЛИРУЮТ (МУЛЬТИКОЛЛИНЕАРНОСТЬ), ТО НУЖНО ЛИБО УДАЛИТЬ ОДИН ИЗ ПРИЗНАКОВ, ЛИБО СОЗДАТЬ НОВЫЙ ПРИЗНАК НА ИХ ОСНОВЕ И ИХ УДАЛИТЬ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-JoioCT0sqG"
      },
      "source": [
        "### 1.3. Подготовка датасета под разные задачи\n",
        "\n",
        "Cравнение подготовки\n",
        "\n",
        "| Модель                  | Масштабирование | Корреляция критична | Выбросы критичны | Feature Engineering рекомендуем                  |\n",
        "| ----------------------- | --------------- | ------------------- | ---------------- | ------------------------------------------------ |\n",
        "| Линейная регрессия      | Да              | Да                  | Да               | Полиномы, логарифмы, отношения                   |\n",
        "| Random Forest Regressor | Нет             | Нет                 | Нет              | Соотношения, интеракции                          |\n",
        "| XGBoost Regressor       | Нет             | Нет                 | Нет              | Соотношения, интеракции, логарифмы (опционально) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmY4Kuz40yKd"
      },
      "source": [
        "#### 1.3.1 Линейная регрессия (Linear Regression / Ridge / Lasso)\n",
        "\n",
        "**Особенности модели:**\n",
        "\n",
        "- Чувствительна к масштабу признаков и мультиколлинеарности.\n",
        "- Чувствительна к выбросам."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGCXGuXr2DiE"
      },
      "source": [
        "Сделаем копию датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVARP3Uh2GWM"
      },
      "outputs": [],
      "source": [
        "df_linear = df_processed.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTe7MGKYGlfl"
      },
      "source": [
        "Далее избавимся от выбросов. Удалим крайние 1% у функции распределения\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLmRaJUIGqhi"
      },
      "outputs": [],
      "source": [
        "lower_limit = df_linear['exam_score'].quantile(0.01)\n",
        "upper_limit = df_linear['exam_score'].quantile(0.99)\n",
        "\n",
        "df_linear = df_linear[(df_linear['exam_score'] >= lower_limit) &\n",
        "                 (df_linear['exam_score'] <= upper_limit)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQTyAv0NHV5G"
      },
      "source": [
        "Убедимся что выбросов нет"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk_kbwLoHZnr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(7, 6))\n",
        "\n",
        "ax.boxplot(\n",
        "    df_linear['exam_score'],\n",
        "    patch_artist=True,\n",
        "    boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
        "    medianprops=dict(color='darkred', linewidth=2)\n",
        ")\n",
        "\n",
        "ax.set_title('Ящик с усами')\n",
        "ax.set_ylabel('Экзаменационный балл')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC8Mbz_V2Lgy"
      },
      "source": [
        "Рассмотрим скошенность данных\n",
        "\n",
        "***Определение 2: Скошенность – это мера асимметрии распределения признака***\n",
        "\n",
        "* **Скошенность > 0 (положительная):** Хвост распределения тянется вправо (большие значения встречаются реже).\n",
        "\n",
        "* **Скошенность < 0 (отрицательная):** Хвост распределения тянется влево (малые значения редки).\n",
        "\n",
        "* **Скошенность ≈ 0:** Практически нормальное распределение (симметричное).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrf4Zf5K2slT"
      },
      "outputs": [],
      "source": [
        "skew_values = df_linear.select_dtypes(include='float').skew()\n",
        "skew_values_hard = skew_values[abs(skew_values) > 0.2]  # выделяем сильноскошенные данные\n",
        "print(skew_values_hard)  # сильно скошенные\n",
        "\n",
        "skew_columns_hard = list(skew_values_hard.index)\n",
        "print(f'Скошенные столбцы {skew_columns_hard}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2o28CxuYLoU"
      },
      "outputs": [],
      "source": [
        "df_processed.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PLIbZrx3IRm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def draw_skew(df: pd.DataFrame, n_cols=4):\n",
        "    numeric_cols = df.select_dtypes(include='float').columns\n",
        "    n_rows = math.ceil(len(numeric_cols) / n_cols)\n",
        "\n",
        "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*4, n_rows*3))\n",
        "    axs = axs.flatten()  # делаем одномерным массивом для удобства\n",
        "\n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        sns.histplot(df[col], kde=True, ax=axs[i])\n",
        "        axs[i].set_title(f'{col}')\n",
        "\n",
        "    for j in range(i+1, len(axs)):\n",
        "        axs[j].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "draw_skew(df_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQRr6WJ8dAa8"
      },
      "source": [
        "**Анализ распределений признаков:**\n",
        "\n",
        "Анализ гистограмм показал, что большинство ключевых признаков имеют приемлемые для регрессионного моделирования распределения. Академические признаки (study_hours_per_day, attendance_percentage, exam_score) и базовые характеристики (sleep_hours) демонстрируют нормальные или близкие к нормальным распределения, что оптимально для линейной регрессии. Lifestyle-признаки (social_media_hours, netflix_hours) показывают умеренную правостороннюю асимметрию с концентрацией малых значений, что типично для поведенческих данных. Некоторые категориальные признаки после кодирования (parental_education_level, internet_quality) имеют бимодальные распределения, указывающие на смешение групп. Демографические признаки (age, mental_health_rating) показывают относительно равномерные распределения. **Критической проблемой является только сильная правосторонняя асимметрия netflix_hours, которая может потребовать логарифмической трансформации.** В целом, качество распределений позволяет применять стандартные регрессионные алгоритмы без существенной предобработки данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLcQkpOJ3_aD"
      },
      "source": [
        "Можно уменьшить влияние скошенности с помошью:\n",
        "\n",
        "- Логарифмирование (Уменьшаем положительный хвост)\n",
        "- Квадратный корень  (сглаживаем умеренные хвосты)\n",
        "- `Box-Cox` или `Yeo-Johnson` трансформации (более гибкие)\n",
        "\n",
        "Мы же просто прологарифмируем :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2ogx_n24Zio"
      },
      "outputs": [],
      "source": [
        "df_linear['netflix_hours'] = np.log1p(df_linear['netflix_hours'])\n",
        "df_linear['attendance_percentage'] = np.log1p(df_linear['attendance_percentage'])\n",
        "draw_skew(df_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCHplr_c8TpR"
      },
      "source": [
        "Посмотрим отдельно зависимости от целевого признака\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMlmp_7p8aNb"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "target = 'exam_score'\n",
        "num_cols = [\n",
        "'study_hours_per_day',\n",
        "'social_media_hours',\n",
        "'netflix_hours',\n",
        "'attendance_percentage',\n",
        "'sleep_hours'\n",
        "]\n",
        "\n",
        "cols = 2\n",
        "rows = int(np.ceil(len(num_cols)/cols))\n",
        "plt.figure(figsize=(12, 4*rows))\n",
        "for i, xcol in enumerate(num_cols, start=1):\n",
        "    ax = plt.subplot(rows, cols, i)\n",
        "    # Scatter + LOWESS\n",
        "    sns.regplot(\n",
        "        data=df, x=xcol, y=target,\n",
        "        lowess=True,\n",
        "        scatter_kws={'alpha':0.3, 's':18},\n",
        "        line_kws={'color':'crimson', 'lw':2},\n",
        "        ax=ax\n",
        "    )\n",
        "    # Линейная линия поверх\n",
        "    sns.regplot(\n",
        "        data=df, x=xcol, y=target,\n",
        "        scatter=False, order=1,\n",
        "        line_kws={'color':'steelblue', 'lw':1.2, 'alpha':0.7},\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(f'{target} vs {xcol}')\n",
        "    ax.grid(alpha=0.25)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfBLoLma5ir_"
      },
      "outputs": [],
      "source": [
        "disc_cols = []\n",
        "for c in df.columns:\n",
        "    if c == target:\n",
        "        continue\n",
        "    if pd.api.types.is_integer_dtype(df[c]):\n",
        "        u = df[c].nunique()\n",
        "        if 3 <= u <= 30:\n",
        "            disc_cols.append(c)\n",
        "print('Дискретные числовые признаки:', disc_cols)\n",
        "\n",
        "def plot_point_ci95(df, xcol, ycol=target):\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.pointplot(\n",
        "        data=df, x=xcol, y=ycol,\n",
        "        estimator=np.mean,\n",
        "        errorbar=('ci', 95),   # доверительный интервал для среднего\n",
        "        color='black'\n",
        "    )\n",
        "    plt.title(f'{ycol} by {xcol}: mean ± 95% CI')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for col in disc_cols:\n",
        "    plot_point_ci95(df, col, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYVPW3b5M7re"
      },
      "source": [
        "Создадим навые признаки\n",
        "1) Учебные часы: центр + квадрат\n",
        "study_hours_c — центрированная линейная часть, снижает коллинеарность с квадратичным членом и стабилизирует коэффициенты.\n",
        "\n",
        "study_hours_c_sq — мягкая нелинейность для эффекта убывающей отдачи на больших значениях учебных часов\n",
        "\n",
        "При желании исходный столбец можно убрать, оставив центрированную версию как линейную компоненту.\n",
        "\n",
        "2) Взаимодействие сна и ментального состояния\n",
        "sleep_mental = sleep_hours × mental_health_rating моделирует совместный восстановительный эффект, который не улавливается простой суммой признаков.\n",
        "\n",
        "Линейная регрессия без взаимодействия предполагает независимые наклоны; этот признак позволяет наклону одного фактора зависеть от уровня другого.\n",
        "\n",
        "3) Влияние\n",
        "study_job = study_hours_per_day × part_time_job_Yes задаёт разные наклоны для работающих и неработающих, отражая реальную модерацию ресурса времени.\n",
        "\n",
        "4) Влияние экранного времени на учебу + нормировка\n",
        "leisure_ratio = (social_media_hours + netflix_hours)/(study_hours_per_day + ε) сворачивает два слабых отрицательных фактора в одну осмысленную метрику «экраны на единицу учебы», уменьшая число сильно связанных столбцов; ε защищает от деления на ноль.\n",
        "\n",
        "attendance_norm = attendance_percentage/100 заменяет процент на долю, упрощая масштаб и регуляризацию; исходный процент можно удалить, чтобы не держать точную линейную копию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt9bVr-O9dKr"
      },
      "outputs": [],
      "source": [
        "def make_features_for_linear(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df_in.copy()\n",
        "\n",
        "    # 1) Учебные часы: оставляем линейный центрированный + один квадратичный\n",
        "    if 'study_hours_per_day' in df.columns:\n",
        "        m = df['study_hours_per_day'].mean()\n",
        "        df['study_hours_c']    = df['study_hours_per_day'] - m\n",
        "        df['study_hours_c_sq'] = df['study_hours_c'] ** 2\n",
        "        df = df.drop(columns=['study_hours_per_day'])\n",
        "\n",
        "    # 2) связь сна с ментальным здоровьем\n",
        "    if {'sleep_hours', 'mental_health_rating'}.issubset(df.columns):\n",
        "        df['sleep_mental'] = df['sleep_hours'] * df['mental_health_rating']\n",
        "\n",
        "    # 3) влияние подработки на время, уделяемое учебе\n",
        "    if {'study_hours_per_day', 'part_time_job_Yes'}.issubset(df.columns):\n",
        "        df['study_job'] = df['study_hours_per_day'] * df['part_time_job_Yes'].astype(int)\n",
        "\n",
        "    # 4) Отношение времени развлечений к времени учебы (одна метрика вместо двух)\n",
        "    if {'social_media_hours', 'netflix_hours', 'study_hours_per_day'}.issubset(df.columns):\n",
        "        eps = 1e-3\n",
        "        df['leisure_ratio'] = (df['social_media_hours'] + df['netflix_hours']) / (df['study_hours_per_day'] + eps)\n",
        "        # По желанию: можно исключить social_media_hours и netflix_hours, чтобы избежать избыточности\n",
        "        df = df.drop(columns=['social_media_hours','netflix_hours'])\n",
        "\n",
        "    # 5) Нормировка посещаемости (замена исходного процента)\n",
        "    if 'attendance_percentage' in df.columns:\n",
        "        df['attendance_norm'] = df['attendance_percentage'] / 100.0\n",
        "        # Для устранения точной линейной копии:\n",
        "        df = df.drop(columns=['attendance_percentage'])\n",
        "\n",
        "    return df\n",
        "df_linear = make_features_for_linear(df_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izkoZbxa-NGC"
      },
      "source": [
        "Разделение датасета на признаки и целевую переменную"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBkNLNRQ-W4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_linear = df_linear.drop(columns=['exam_score'])\n",
        "y_linear = df_linear['exam_score']\n",
        "\n",
        "# Разделение выборки на test/train (20/80)\n",
        "X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(\n",
        "    X_linear, y_linear, test_size=0.2, random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGWQimz3_Efk"
      },
      "source": [
        "Данные для обучения модели нужно стандартизировать\n",
        "\n",
        "***Определение 4: Стандартизация признаков — это метод преобразования числовых признаков так, чтобы они имели среднее значение 0 и стандартное отклонение 1. Это важный шаг в подготовке данных для моделей, чувствительных к масштабу признаков, например линейной регрессии, логистической регрессии, SVM, KNN.***\n",
        "\n",
        "**Как это работает**\n",
        "\n",
        "Для каждого признака $x$ вычисляется:\n",
        "\n",
        "$$\n",
        "x_\\text{scaled} = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "где:\n",
        "\n",
        "* $\\mu$ — среднее значение признака в обучающей выборке,\n",
        "* $\\sigma$ — стандартное отклонение признака.\n",
        "\n",
        "После стандартизации:\n",
        "\n",
        "* Среднее значение нового признака ≈ 0\n",
        "* Стандартное отклонение ≈ 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-hXma_4AKEa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_linear_scaled = scaler.fit_transform(X_train_linear)  # вычисляет среднее и стандартное отклонение (только на train)\n",
        "X_test_linear_scaled = scaler.transform(X_test_linear)  # применяет эти параметры к любым данным (train, test, новые данные)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LpumPhfBGFT"
      },
      "source": [
        "#### 1.3.2 Random Forest Regressor\n",
        "\n",
        "**Особенности модели:**\n",
        "\n",
        "* Не чувствительна к масштабу признаков.\n",
        "* Может обрабатывать сильные корреляции между признаками.\n",
        "* Может использовать категориальные признаки, если они закодированы как числовые.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HF3BAGFCE9O"
      },
      "source": [
        "Сделаем копию датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91HfLSWaCMyh"
      },
      "outputs": [],
      "source": [
        "df_forest = df_processed.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrqF5KjrCUms"
      },
      "source": [
        "Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCEraIqwCXRX"
      },
      "outputs": [],
      "source": [
        "def make_forest_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    df_f = df.copy()\n",
        "\n",
        "    # 1) Квадрат учебных часов — захватывает ускоряющийся рост оценки с ростом study_hours_per_day\n",
        "    if 'study_hours_per_day' in df_f.columns:\n",
        "        df_f['study_hours_sq'] = df_f['study_hours_per_day'] ** 2\n",
        "\n",
        "    # 2) Отклонение сна от оптимума (8 ч) в квадрате —\n",
        "    #    учитывает, что слишком мало и слишком много сна негативно влияет на оценку\n",
        "    if 'sleep_hours' in df_f.columns:\n",
        "        df_f['sleep_dev_sq'] = (df_f['sleep_hours'] - 8.0) ** 2\n",
        "\n",
        "    # 3) Соотношение времени развлечений к учёбе —\n",
        "    #    объединяет социальные медиа и Netflix в одну метрику влияния отвлечений\n",
        "    if {'social_media_hours','netflix_hours','study_hours_per_day'}.issubset(df_f.columns):\n",
        "        df_f['leisure_ratio'] = (\n",
        "            df_f['social_media_hours'] + df_f['netflix_hours']\n",
        "        ) / (df_f['study_hours_per_day'] + 1e-6)\n",
        "\n",
        "    # 4) Взаимодействие сна и учёбы — отражает, что сочетание здорового сна и достаточного времени на учёбу\n",
        "    if {'sleep_hours','study_hours_per_day'}.issubset(df_f.columns):\n",
        "        df_f['sleep_study_inter'] = (\n",
        "            df_f['sleep_hours'] * df_f['study_hours_per_day']\n",
        "        )\n",
        "\n",
        "    return df_f\n",
        "df_forest = make_forest_features(df_forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOULg6ZWCgO0"
      },
      "source": [
        "Разделение выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y68KgamxCh4S"
      },
      "outputs": [],
      "source": [
        "X_forest = df_forest.drop(columns=['exam_score'])\n",
        "y_forest = df_forest['exam_score']\n",
        "\n",
        "X_train_forest, X_test_forest, y_train_forest, y_test_forest = train_test_split(\n",
        "    X_forest, y_forest, test_size=0.2, random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZyJlzLvC6zq"
      },
      "source": [
        "#### 1.3.3 XGBoost\n",
        "\n",
        "**Особенности модели:**\n",
        "\n",
        "* Градиентный бустинг деревьев.\n",
        "* Не чувствителен к масштабу.\n",
        "* Может обрабатывать коррелированные признаки, но слишком много слабых признаков может замедлить обучение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NzeiuD0DT50"
      },
      "source": [
        "Сделаем копию датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGHKTS_1DaF9"
      },
      "outputs": [],
      "source": [
        "df_xgboost = df_processed.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDJj3lBsgYLY"
      },
      "source": [
        "Чистим от выбросов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kBHCfU8gbXD"
      },
      "outputs": [],
      "source": [
        "lower_limit = df_xgboost['exam_score'].quantile(0.01)\n",
        "upper_limit = df_xgboost['exam_score'].quantile(0.99)\n",
        "\n",
        "df_xgboost = df_xgboost[(df_xgboost['exam_score'] >= lower_limit) &\n",
        "                 (df_xgboost['exam_score'] <= upper_limit)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P508enF6DjF4"
      },
      "source": [
        "Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjB83JZRDiNz"
      },
      "outputs": [],
      "source": [
        "def make_xgb_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Обоснованные признаки для XGBoostRegressor:\n",
        "    учтено, что модель не чувствительна к масштабу,\n",
        "    но слишком много слабых признаков замедляет обучение.\n",
        "    \"\"\"\n",
        "    df_xgb = df.copy()\n",
        "\n",
        "    # 1) Логарифм учебных часов — ловит убывающий эффект при очень долгой учёбе\n",
        "    if 'study_hours_per_day' in df_xgb.columns:\n",
        "        df_xgb['study_hours_log'] = np.log1p(df_xgb['study_hours_per_day'])\n",
        "\n",
        "    # 2) Отклонение сна от оптимума (8 ч)\n",
        "    if 'sleep_hours' in df_xgb.columns:\n",
        "        df_xgb['sleep_dev'] = (df_xgb['sleep_hours'] - 8.0).abs()\n",
        "\n",
        "    # 3) Соотношение развлечений к учёбе\n",
        "    if {'social_media_hours','netflix_hours','study_hours_per_day'}.issubset(df_xgb.columns):\n",
        "        df_xgb['leisure_ratio'] = (\n",
        "            df_xgb['social_media_hours'] + df_xgb['netflix_hours']\n",
        "        ) / (df_xgb['study_hours_per_day'] + 1e-6)\n",
        "\n",
        "    # 4) Интерактивный признак посещаемость × учёба\n",
        "    if {'attendance_percentage','study_hours_per_day'}.issubset(df_xgb.columns):\n",
        "        df_xgb['attendance_study_inter'] = (\n",
        "            df_xgb['attendance_percentage'] * df_xgb['study_hours_per_day']\n",
        "        )\n",
        "\n",
        "    return df_xgb\n",
        "\n",
        "df_xgboost = make_xgb_features(df_xgboost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtppWIc5Dzw4"
      },
      "source": [
        "Разделение выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olxuyViVD2L1"
      },
      "outputs": [],
      "source": [
        "X_xgboost = df_xgboost.drop(columns=['exam_score'])\n",
        "y_xgboost = df_xgboost['exam_score']\n",
        "\n",
        "X_train_xgboost, X_test_xgboost, y_train_xgboost, y_test_xgboost = train_test_split(\n",
        "    X_xgboost, y_xgboost, test_size=0.2, random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64U5EhUNE00s"
      },
      "source": [
        "## 2. Создание метрик\n",
        "\n",
        "1. **MSE (Mean Squared Error)** – средняя квадратичная ошибка:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "* Чем меньше, тем лучше.\n",
        "* Чувствительна к выбросам (квадрат ошибки усиливает влияние больших отклонений).\n",
        "\n",
        "2. **RMSE (Root Mean Squared Error)** – корень из MSE:\n",
        "\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
        "$$\n",
        "\n",
        "* В тех же единицах, что и целевая переменная.\n",
        "* Легче интерпретировать.\n",
        "\n",
        "3. **MAE (Mean Absolute Error)** – средняя абсолютная ошибка:\n",
        "\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "\n",
        "* Менее чувствительна к выбросам, показывает «среднюю ошибку» в исходных единицах.\n",
        "\n",
        "4. **R² (коэффициент детерминации)**:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "* 1 → идеальное предсказание, 0 → модель не лучше среднего, <0 → хуже среднего.\n",
        "\n",
        "**Как интерпретировать**\n",
        "\n",
        "| Метрика | Как читать                    | Что значит для анализа                                                   |\n",
        "| ------- | ----------------------------- | ------------------------------------------------------------------------ |\n",
        "| MSE     | Чем меньше, тем точнее        | Показывает среднюю квадратичную ошибку. Выбросы сильно влияют.           |\n",
        "| RMSE    | В тех же единицах, что и цель | Удобно для прямой интерпретации ошибок.                                  |\n",
        "| MAE     | Средняя абсолютная ошибка     | Устойчивее к выбросам, показывает среднюю фактическую ошибку.            |\n",
        "| R²      | 0–1 (или <0)                  | 1 — идеальное совпадение, 0 — предсказывает среднее, <0 — хуже среднего. |\n",
        "\n",
        "**Пример анализа:**\n",
        "\n",
        "* Если RMSE и MAE сильно отличаются → есть выбросы.\n",
        "* Если R² близок к 1 → модель хорошо объясняет вариацию данных.\n",
        "* Можно сравнивать модели: линейная, RF, XGBoost. Та, у которой меньше RMSE/MAE и выше R² — более точная.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2RZ0UGIFqoy"
      },
      "outputs": [],
      "source": [
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mse(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def r2(y_true, y_pred):\n",
        "    return 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))\n",
        "\n",
        "def get_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        'MSE': mse(y_true, y_pred),\n",
        "        'RMSE': rmse(y_true, y_pred),\n",
        "        'MAE': mae(y_true, y_pred),\n",
        "        'R2': r2(y_true, y_pred)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX0RJ_ZyHSx-"
      },
      "source": [
        "Прежде чем начнем обучать, создадим `DataFrame` для снятия метрик"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKowHM81GEIF"
      },
      "source": [
        "## 3. Базовая модель линейной регрессии (аналитическое решение)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6AsZHA3Hfwn"
      },
      "outputs": [],
      "source": [
        "# Создание линейной модели\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR9jUj_kJJcq"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ds0XAVuGp2o"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr.predict(X_test_linear_scaled)\n",
        "lr_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQTAck1cJ-UV"
      },
      "source": [
        "## 4. Улучшенная версия линейной регрессии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9IXovgjK96B"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxwqJaZcKEy5"
      },
      "source": [
        "### 4.1. Градиентный спуск (симуляция)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puI0wEHuKjEC"
      },
      "outputs": [],
      "source": [
        "# Создание линейной модели градиентного спуска\n",
        "lr_gd = SGDRegressor(\n",
        "    max_iter=2000,               # максимум итераций\n",
        "    tol=1e-6,                    # остановка, когда улучшение < tol\n",
        "    learning_rate='constant',  # тип изменения шага\n",
        "    eta0=0.01,                   # стартовый шаг\n",
        "    penalty='l2',                # регуляризация L2 (Ridge)\n",
        "    shuffle=False,               # важный момент: не перемешиваем данные, чтобы был настоящий GD\n",
        "    random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZfW3Nx-K8Cy"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr_gd.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJVqu9dELH8u"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr_gd.predict(X_test_linear_scaled)\n",
        "lr_gd_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_gd_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGgLysdfMqUK"
      },
      "source": [
        "### 4.2. Стохастический градиентный спуск"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RFvRH2WMp9n"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "lr_sgd = SGDRegressor(\n",
        "    max_iter=2000,                # максимум итераций\n",
        "    tol=1e-6,                     # остановка, когда улучшение < tol\n",
        "    learning_rate='invscaling',   # тип изменения шага\n",
        "    eta0=0.01,                    # стартовый шаг\n",
        "    penalty='l1',                 # регуляризация L2 (Ridge)\n",
        "    random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_Htp_taNAY8"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr_sgd.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kymCnNFVNHKx"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr_sgd.predict(X_test_linear_scaled)\n",
        "lr_sgd_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_sgd_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xKy6vnKNbEm"
      },
      "source": [
        "### 4.3. Линейная модель с регуляризацией Rigde (L2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJk4sqFkNm-q"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "from sklearn.linear_model import Ridge\n",
        "lr_ridge = Ridge(alpha=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axm_sBm5Nsos"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr_ridge.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7zPRi_SNv63"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr_ridge.predict(X_test_linear_scaled)\n",
        "lr_ridge_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_ridge_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlfBXP0uOF5X"
      },
      "source": [
        "### 4.3. Линейная модель с регуляризацией Lasso (L1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02xQ4Cv1OFde"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "from sklearn.linear_model import Lasso\n",
        "lr_lasso = Lasso(alpha=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFNHEyffOPaE"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr_lasso.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic-1jtCGOWsu"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr_lasso.predict(X_test_linear_scaled)\n",
        "lr_lasso_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_lasso_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgpha61_OlfO"
      },
      "source": [
        "### 4.4. Линейная модель с регуляризацией ElasticNet (комбинация L1+L2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLm0wREpOsxV"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "from sklearn.linear_model import ElasticNet\n",
        "lr_enet = ElasticNet(alpha=0.01, l1_ratio=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek5RqvtoO0Ed"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "lr_enet.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYHBl6WYO3Do"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = lr_enet.predict(X_test_linear_scaled)\n",
        "lr_enet_metrics = get_metrics(y_test_linear, y_pred)\n",
        "lr_enet_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6PsvHh6PJjI"
      },
      "source": [
        "## 4. Случайный лес регрессор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOHmsirfPXDc"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=200,      # количество деревьев\n",
        "    max_depth=None,        # глубина деревьев\n",
        "    random_state=RND_SEED,\n",
        "    n_jobs=-1              # использовать все ядра процессора\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cN3MjnQPqtz"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "rf.fit(X_train_forest, y_train_forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN39XWpJP6WY"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = rf.predict(X_test_forest)\n",
        "rf_metrics = get_metrics(y_test_forest, y_pred)\n",
        "rf_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDTQyzs2QPMr"
      },
      "source": [
        "## 5. Градиентный бустинг (XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_PmL1JsQZ3r"
      },
      "outputs": [],
      "source": [
        "# Инициализация модели\n",
        "import xgboost\n",
        "xgb = xgboost.XGBRegressor(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=RND_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OvwsLeKQnwg"
      },
      "outputs": [],
      "source": [
        "# Обучение\n",
        "xgb.fit(X_train_xgboost, y_train_xgboost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xECYxKaQz_L"
      },
      "outputs": [],
      "source": [
        "# Прогонка и метрики\n",
        "y_pred = xgb.predict(X_test_xgboost)\n",
        "xgb_metrics = get_metrics(y_test_xgboost, y_pred)\n",
        "xgb_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUAYVo5hRqVW"
      },
      "source": [
        "## 5. Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubh55e8KRzES"
      },
      "source": [
        "### 5.1 Linear Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gouxft_jUMke"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (Linear Regressor)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um6d8vUbSzAX"
      },
      "source": [
        "### 5.2 Linear Regression (GD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feTc7mVLVKGO"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr_gd.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (GD Regressor)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdnODQwoS0Me"
      },
      "source": [
        "### 5.3 Linear Regression (SGD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu2sbaoCV8PC"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr_sgd.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (SGD Regressor)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ1bh_vYS1YR"
      },
      "source": [
        "### 5.4 Linear Regression (Rigde)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C718GqOWBau"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr_ridge.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (LR ridge)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQPAyuZwS2kW"
      },
      "source": [
        "### 5.5 Linear Regression (Lasso)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b26ALP4sWO-5"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr_lasso.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (LR lasso)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0-G2Cj_S3pU"
      },
      "source": [
        "### 5.6 Linear Regression (ElasticNet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3ynG9h8WTnj"
      },
      "outputs": [],
      "source": [
        "importance = pd.DataFrame({\n",
        "    'Feature': X_train_linear.columns,\n",
        "    'Coefficient': lr_enet.coef_\n",
        "}).sort_values(by='Coefficient', key=abs, ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(\n",
        "    x='Coefficient',\n",
        "    y='Feature',\n",
        "    data=importance,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title('Feature Importance (LR elastic_net)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuYh063US4pv"
      },
      "source": [
        "### 5.7 Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWdZTbepTh1X"
      },
      "outputs": [],
      "source": [
        "feature_importances = pd.Series(rf.feature_importances_, index=X_forest.columns)\n",
        "feature_importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=feature_importances.values, y=feature_importances.index)\n",
        "plt.title(\"Feature Importance (Random Forest)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_zFrwKS5si"
      },
      "source": [
        "### 5.6 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZas38yuTF-t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "xgboost.plot_importance(xgb, importance_type='weight', max_num_features=10)\n",
        "plt.title(\"Feature Importance (XGBoost)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPfLMeh5YXY5"
      },
      "source": [
        "## 8. Написание своих реализаций (классы)\n",
        "\n",
        "Напишите свои классы реализации:\n",
        "\n",
        "- LR\n",
        "- LR + GD\n",
        "- LR + SGD\n",
        "- *Random Forest (не обязательно)\n",
        "- *Gradient Boosting Regressor (не обязательно)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRjQiloQhY1P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class CustomLinearRegression:\n",
        "    def __init__(self, method='analytical', random_state=None):\n",
        "        self.method = method\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "        self.random_state = random_state\n",
        "        self._rng = np.random.default_rng(random_state)\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_numpy(X, y):\n",
        "        Xn = X.values if hasattr(X, 'values') else X\n",
        "        yn = y.values.ravel() if hasattr(y, 'values') else np.ravel(y)\n",
        "        return Xn.astype(float), yn.astype(float)\n",
        "\n",
        "    # Аналитическое решение\n",
        "    def analytical_solution(self, X, y):\n",
        "        X, y = self._to_numpy(X, y)\n",
        "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "        try:\n",
        "            XTX_inv = np.linalg.inv(X_with_bias.T @ X_with_bias)\n",
        "            weights_with_bias = XTX_inv @ X_with_bias.T @ y\n",
        "        except np.linalg.LinAlgError:\n",
        "            weights_with_bias = np.linalg.pinv(X_with_bias) @ y\n",
        "        self.bias = float(weights_with_bias[0])\n",
        "        self.weights = weights_with_bias[1:].astype(float)\n",
        "\n",
        "    # Градиентный спуск\n",
        "    def gradient_descent(self, X, y, learning_rate=0.01, max_iterations=1000):\n",
        "        X, y = self._to_numpy(X, y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features, dtype=float)\n",
        "        self.bias = 0.0\n",
        "        self.cost_history = []\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            y_pred = X @ self.weights + self.bias\n",
        "            error = y_pred - y\n",
        "            cost = np.mean(error ** 2)\n",
        "            self.cost_history.append(float(cost))\n",
        "\n",
        "            dw = (2.0 / n_samples) * (X.T @ error)\n",
        "            db = (2.0 / n_samples) * np.sum(error)\n",
        "\n",
        "            self.weights -= learning_rate * dw\n",
        "            self.bias    -= learning_rate * db\n",
        "\n",
        "            if i > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < 1e-8:\n",
        "                break\n",
        "\n",
        "    # Стохастический градиентный спуск\n",
        "    def stochastic_gradient_descent(self, X, y, learning_rate=0.01, max_iterations=1000, batch_size=32):\n",
        "        X, y = self._to_numpy(X, y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features, dtype=float)\n",
        "        self.bias = 0.0\n",
        "        self.cost_history = []\n",
        "\n",
        "        for epoch in range(max_iterations):\n",
        "            indices = self._rng.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            total_cost, num_batches = 0.0, 0\n",
        "\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                end_idx = min(i + batch_size, n_samples)\n",
        "                X_batch = X_shuffled[i:end_idx]\n",
        "                y_batch = y_shuffled[i:end_idx]\n",
        "\n",
        "                y_pred = X_batch @ self.weights + self.bias\n",
        "                error = y_pred - y_batch\n",
        "                cost = np.mean(error ** 2)\n",
        "\n",
        "                m = X_batch.shape[0]\n",
        "                dw = (2.0 / m) * (X_batch.T @ error)\n",
        "                db = (2.0 / m) * np.sum(error)\n",
        "\n",
        "                self.weights -= learning_rate * dw\n",
        "                self.bias    -= learning_rate * db\n",
        "\n",
        "                total_cost += float(cost)\n",
        "                num_batches += 1\n",
        "\n",
        "            self.cost_history.append(total_cost / max(num_batches, 1))\n",
        "\n",
        "            if epoch > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < 1e-8:\n",
        "                break\n",
        "\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        if self.method == 'analytical':\n",
        "            self.analytical_solution(X, y)\n",
        "        elif self.method == 'gradient_descent':\n",
        "            self.gradient_descent(X, y, **kwargs)\n",
        "        elif self.method == 'sgd':\n",
        "            self.stochastic_gradient_descent(X, y, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"Неподдерживаемый метод\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.weights is None:\n",
        "            raise ValueError(\"Сначала вызовите fit()\")\n",
        "        Xn = X.values if hasattr(X, 'values') else X\n",
        "        Xn = Xn.astype(float)\n",
        "        return Xn @ self.weights + self.bias\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_true = y.values.ravel() if hasattr(y, 'values') else np.ravel(y)\n",
        "        y_pred = self.predict(X)\n",
        "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        return 1.0 - ss_res / ss_tot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G65XSeTp-VT"
      },
      "outputs": [],
      "source": [
        "custom_lr = CustomLinearRegression(\"analytical\")\n",
        "custom_lr.fit(X_train_linear_scaled, y_train_linear)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w88HjHcfuzhi"
      },
      "outputs": [],
      "source": [
        "y_pred = custom_lr.predict(X_test_linear_scaled)\n",
        "custom_lr_metrics = get_metrics(y_test_linear, y_pred);\n",
        "custom_lr_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvsa-K0qv1Vr"
      },
      "outputs": [],
      "source": [
        "custom_lr = CustomLinearRegression(\"gradient_descent\")\n",
        "custom_lr.fit(X_train_linear_scaled, y_train_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZH_x1h_v_GT"
      },
      "outputs": [],
      "source": [
        "y_pred = custom_lr.predict(X_test_linear_scaled)\n",
        "custom_lr_gd_metrics = get_metrics(y_test_linear, y_pred);\n",
        "custom_lr_gd_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVuWTt5wTxU"
      },
      "outputs": [],
      "source": [
        "custom_lr = CustomLinearRegression(\"sgd\")\n",
        "custom_lr.fit(X_train_linear_scaled, y_train_linear)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWGH0B3qw3mk"
      },
      "outputs": [],
      "source": [
        "y_pred = custom_lr.predict(X_test_linear_scaled)\n",
        "custom_lr_sgd_metrics = get_metrics(y_test_linear, y_pred);\n",
        "custom_lr_sgd_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F8H7I4JWqMO"
      },
      "source": [
        "## 9. Итоги\n",
        "\n",
        "Что сделать?\n",
        "\n",
        "1. Сгрупировать все метрики, и выяснить, какой метод сработал лучше всего и почему?\n",
        "2. Ответить на вопросы:\n",
        "\n",
        "    1. Что такое регрессия и чем она отличается от классификации?\n",
        "    2. Какова целевая переменная в задаче регрессии?\n",
        "    3. Зачем нужно масштабирование признаков перед обучением линейной регрессии?\n",
        "    4. Что означает коэффициент признака в линейной регрессии?\n",
        "    5. Что такое MSE, RMSE, MAE и R², и чем они отличаются?\n",
        "    6. В чем разница между Ridge и Lasso регуляризацией?\n",
        "    7. Почему деревья решений и Random Forest не требуют стандартизации признаков?\n",
        "    8. Что такое мультиколлинеарность и почему она мешает линейной регрессии?\n",
        "    9. Как можно уменьшить влияние выбросов на линейную регрессию?\n",
        "    10. Какие гиперпараметры наиболее важны для Random Forest Regressor?\n",
        "    11. Какие гиперпараметры наиболее важны для XGBoost в задаче регрессии?\n",
        "    12. Что значит глубина дерева (max\\_depth) и как она влияет на модель?\n",
        "    13. Зачем нужен `learning_rate` в градиентном бустинге?\n",
        "    14. Как можно оценить важность признаков (feature importance) в линейной регрессии, случайном лесу и XGBoost?\n",
        "    15. Почему XGBoost часто работает лучше, чем Random Forest, на структурированных данных?\n",
        "    16. Что такое переобучение и как его можно выявить на графике обучения?\n",
        "    17. Как работает метод ансамблирования в Random Forest (bagging)?\n",
        "    18. В чем отличие бустинга от бэггинга?\n",
        "    19. Какие способы feature engineering можно применить к вашему датасету?\n",
        "    20. Как использовать кросс-валидацию для подбора гиперпараметров моделей регрессии?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vPq85IPXeA3"
      },
      "source": [
        "#### 9.1. Группировка метрик"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-oBQuOkXq7W"
      },
      "outputs": [],
      "source": [
        "# code here\n",
        "\n",
        "method2metrics = {\n",
        "    'LinearRegression':              lr_metrics,\n",
        "    'Gradient descend':              lr_gd_metrics,\n",
        "    'SGDRegressor':                  lr_sgd_metrics,\n",
        "    'Ridge':                         lr_ridge_metrics,\n",
        "    'Lasso':                         lr_lasso_metrics,\n",
        "    'ElasticNet':                    lr_enet_metrics,\n",
        "    'RandomForestRegressor':         rf_metrics,\n",
        "    'XGBRegressor':                  xgb_metrics,\n",
        "\n",
        "    'Custom LR (analytical)':        custom_lr_metrics,\n",
        "    'Custom LR + GD':                custom_lr_gd_metrics,\n",
        "    'Custom LR + SGD':               custom_lr_sgd_metrics,\n",
        "}\n",
        "\n",
        "# 2) Нормализуем типы и оставляем ключевые метрики\n",
        "norm = {}\n",
        "for method, md in method2metrics.items():\n",
        "    if md is None:\n",
        "        continue\n",
        "    norm[method] = {\n",
        "        k: float(v) for k, v in md.items()\n",
        "        if k in ('MSE', 'RMSE', 'MAE', 'R2')\n",
        "    }\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(norm, orient='index')\n",
        "desired = ['MSE', 'RMSE', 'MAE', 'R2']\n",
        "cols = [c for c in desired if c in df.columns] + [c for c in df.columns if c not in desired]\n",
        "df = df[cols].sort_values('MSE')\n",
        "df.index.name = 'method'\n",
        "\n",
        "\n",
        "df.round(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJhehvhx0OWQ"
      },
      "source": [
        "Вывод: данные ведут себя почти линейно, поэтому регуляризованные линейные модели закономерно обгоняют деревья, а отсутствие тщательного тюнинга ухудшило результаты XGBoost и случайного леса. Небольшой выигрыш ElasticNet объясним совместной L1+L2 регуляризацией, которая одновременно стабилизирует коэффициенты при мультиколлинеарности и делает мягкий отбор признаков.\n",
        "\n",
        "Дерево - кусочно‑постоянная аппроксимация, и чтобы точно повторить почти прямую зависимость, нужны глубокие деревья. при типовых настройках получается недообучение относительно простой линейной функции.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCISA5SAXnie"
      },
      "source": [
        "#### 9.2. Ответы на вопросы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDdlS6IeXul9"
      },
      "source": [
        "1. Что такое регрессия и чем она отличается от классификации?\n",
        "\n",
        "Регрессия — это задача машинного обучения с учителем для предсказания непрерывных числовых значений. В основе регрессионного анализа лежит построение функциональной зависимости между независимыми переменными (признаками) и зависимой переменной (целевой). Классификация же предсказывает категориальные переменные — принадлежность объекта к определенному классу.\n",
        "\n",
        "Ключевые отличия: в регрессии целевая переменная может принимать бесконечное множество значений (цены домов, температура, экзаменационные баллы как в вашей ЛР), в классификации — конечное множество дискретных категорий (spam/not spam, диагнозы болезней). Метрики также различаются: для регрессии используют MSE, RMSE, MAE, R², для классификации — accuracy, precision, recall, F1-score.\n",
        "\n",
        "2. Какова целевая переменная в задаче регрессии?\n",
        "\n",
        "Целевая переменная в регрессии — это количественная переменная, которую мы пытаемся предсказать. Она должна быть измеримой и принимать числовые значения из непрерывного или дискретного множества. В нашей лабораторной работе целевая переменная — exam_score.\n",
        "\n",
        "Другие примеры: стоимость недвижимости, температура воздуха, объем продаж, доходы компании, количество клиентов. Важно, чтобы целевая переменная была численно осмысленной — разность между значениями должна иметь интерпретацию (разница между 80 и 70 баллами такая же, как между 60 и 50).\n",
        "\n",
        "3. Зачем нужно масштабирование признаков перед обучением линейной регрессии?\n",
        "\n",
        "Масштабирование критично для линейных моделей, поскольку признаки имеют разные единицы измерения и диапазоны значений. В нашем датасете: age (17-24), study_hours_per_day (0-8.3), attendance_percentage (56-100). Без масштабирования признаки с большими значениями будут доминировать в процессе оптимизации.\n",
        "\n",
        "Проблемы без масштабирования: градиентный спуск сходится медленно или вообще не сходится, коэффициенты становятся несопоставимыми по величине, численная неустойчивость алгоритмов. StandardScaler приводит все признаки к стандартному нормальному распределению (μ=0, σ=1), что обеспечивает равные условия для всех переменных в процессе оптимизации и правильную интерпретацию коэффициентов.\n",
        "\n",
        "4. Что означает коэффициент признака в линейной регрессии?\n",
        "\n",
        "Коэффициент показывает изменение целевой переменной при изменении соответствующего признака на одну единицу, при условии, что все остальные признаки остаются неизменными. В нашей ЛР коэффициент для study_hours_c равен ~12.5, что означает: каждый дополнительный час учебы увеличивает экзаменационный балл в среднем на 12.5 пунктов.\n",
        "\n",
        "5. Что такое MSE, RMSE, MAE и R², и чем они отличаются?\n",
        "\n",
        "MSE (Mean Squared Error) = (1/n) × Σ(y_true - y_pred)² — средняя квадратичная ошибка. Сильно штрафует большие ошибки из-за возведения в квадрат, единицы измерения — квадрат целевой переменной.\n",
        "\n",
        "RMSE (Root Mean Squared Error) = √MSE — корень из MSE. Возвращает ошибку в исходных единицах (баллы), легко интерпретируется. RMSE=5.26 означает типичную ошибку ±5.26 балла.\n",
        "\n",
        "MAE (Mean Absolute Error) = (1/n) × Σ|y_true - y_pred| — средняя абсолютная ошибка. Устойчива к выбросам, все ошибки весят одинаково.\n",
        "\n",
        "R² (коэффициент детерминации) = 1 - SS_res/SS_tot — доля объясненной дисперсии. R²=0.891 в нашей ЛР означает, что модель объясняет 89.1% вариации экзаменационных баллов.\n",
        "\n",
        "6. В чем разница между Ridge и Lasso регуляризацией?\n",
        "\n",
        "Ridge (L2-регуляризация) добавляет к функции потерь штраф α×Σβᵢ² (сумма квадратов коэффициентов). Уменьшает коэффициенты к нулю, но никогда не обнуляет их полностью. Эффективна при мультиколлинеарности — распределяет веса между коррелирующими признаками.\n",
        "\n",
        "Lasso (L1-регуляризация) добавляет штраф α×Σ|βᵢ| (сумма абсолютных значений). Может полностью обнулять коэффициенты, выполняя автоматический отбор признаков. Создает разреженные модели, где многие коэффициенты равны нулю.\n",
        "\n",
        "7. Почему деревья решений и Random Forest не требуют стандартизации признаков?\n",
        "\n",
        "Деревья принимают решения на основе пороговых значений и упорядочивания, а не расстояний или линейных комбинаций. Алгоритм выбирает лучшее разбиение по принципу \"если study_hours > 4, то...\", где важен только относительный порядок значений, а не их абсолютная величина.\n",
        "\n",
        "Инвариантность к монотонным преобразованиям: умножение признака на константу или добавление константы не меняет результат разбиения. Дерево одинаково разделит данные по признаку со значениями. Random Forest наследует это свойство, поскольку состоит из деревьев. Это существенное преимущество tree-based моделей — они работают с разнородными признаками без предварительной обработки.\n",
        "\n",
        "8. Что такое мультиколлинеарность и почему она мешает линейной регрессии?\n",
        "\n",
        "Мультиколлинеарность — сильная корреляция между предикторами, когда один признак можно выразить через линейную комбинацию других.\n",
        "\n",
        "Проблемы для линейной регрессии: коэффициенты становятся неустойчивыми — малые изменения в данных приводят к резким изменениям весов; затрудняется интерпретация — неясно, какой признак действительно важен; численная неустойчивость — матрица X^T×X становится плохо обусловленной или даже вырожденной.\n",
        "\n",
        "9. Как можно уменьшить влияние выбросов на линейную регрессию?\n",
        "\n",
        "Методы обработки выбросов: 1) Удаление — исключение объектов за пределами 1-99 перцентилей (как в нашей ЛР); 2) Обрезание (clipping) — ограничение экстремальных значений; 3) Трансформация данных — логарифмирование, Box-Cox преобразование для снижения асимметрии.\n",
        "\n",
        "10. Какие гиперпараметры наиболее важны для Random Forest Regressor?\n",
        "\n",
        "n_estimators — количество деревьев в лесу. В вашей ЛР используется 200. Больше деревьев = выше качество, но медленнее обучение. Обычно 100-1000, после определенного значения улучшение незначительно.\n",
        "\n",
        "max_depth — максимальная глубина каждого дерева. Контролирует сложность: глубокие деревья могут переобучаться, мелкие — недообучаться. None (по умолчанию) означает деревья растут до чистых листьев.\n",
        "\n",
        "min_samples_split — минимальное количество образцов для разбиения узла (по умолчанию 2). min_samples_leaf — минимум образцов в листе (по умолчанию 1). Эти параметры контролируют переобучение.\n",
        "\n",
        "max_features — количество признаков для рассмотрения в каждом разбиении. По умолчанию для регрессии — все признаки, но √n_features часто работает лучше. random_state — воспроизводимость результатов.\n",
        "\n",
        "11. Какие гиперпараметры наиболее важны для XGBoost в задаче регрессии?\n",
        "\n",
        "n_estimators — количество деревьев (в вашей ЛР 500). Больше итераций = лучше качество, но выше риск переобучения. learning_rate (eta) — скорость обучения (0.05 в ЛР). Малые значения требуют больше деревьев, но дают стабильное обучение.\n",
        "\n",
        "max_depth — глубина деревьев (4 в ЛР). XGBoost обычно использует неглубокие деревья (3-6), в отличие от Random Forest. subsample — доля образцов для обучения каждого дерева (0.8). Уменьшает переобучение через случайность.\n",
        "\n",
        "colsample_bytree — доля признаков для каждого дерева (0.8). reg_alpha (L1) и reg_lambda (L2) — регуляризация для предотвращения переобучения. early_stopping_rounds — остановка при отсутствии улучшения на валидации.\n",
        "\n",
        "Стратегия настройки: начать с learning_rate=0.1, подобрать n_estimators, затем уменьшить learning_rate и увеличить n_estimators.\n",
        "\n",
        "12. Что значит глубина дерева (max_depth) и как она влияет на модель?\n",
        "\n",
        "max_depth определяет максимальное количество уровней от корня до самого глубокого листа. Это ключевой параметр сложности модели — более глубокие деревья могут выучить более сложные паттерны, но склонны к переобучению.\n",
        "\n",
        "Влияние на модель: мелкие деревья (depth=2-3) создают простые правила, могут недообучаться; глубокие деревья (depth>10) запоминают специфичные комбинации признаков из обучающей выборки, плохо генерализуют.\n",
        "\n",
        "В Random Forest: каждое дерево может быть глубоким (часто без ограничений), поскольку усреднение по многим деревьям снижает переобучение. В XGBoost: обычно используются неглубокие деревья (3-6 уровней), так как бустинг итеративно улучшает предсказания.\n",
        "\n",
        "Выбор оптимальной глубины: кросс-валидация, мониторинг метрик на валидационной выборке.\n",
        "\n",
        "13. Зачем нужен learning_rate в градиентном бустинге?\n",
        "\n",
        "Learning_rate (скорость обучения) контролирует вклад каждого нового дерева в финальное предсказание. Формула обновления: F_new = F_old + learning_rate × h(x), где h(x) — предсказание нового дерева.\n",
        "\n",
        "При высоком learning_rate (близко к 1): быстрое обучение, но высокий риск переобучения и нестабильность. При низком learning_rate (0.01-0.1): медленное, но стабильное обучение, лучшая генерализация, требует больше деревьев.\n",
        "\n",
        "В нашей ЛР: learning_rate=0.05 — консервативное значение, обеспечивающее баланс между скоростью и качеством. Trade-off: низкий learning_rate + много деревьев часто дает лучшие результаты.\n",
        "\n",
        "Адаптивные стратегии: начать с высокого значения, постепенно уменьшать; использовать early stopping для автоматической остановки.\n",
        "\n",
        "14. Как можно оценить важность признаков (feature importance) в линейной регрессии, случайном лесу и XGBoost?\n",
        "\n",
        "Линейная регрессия: важность определяется абсолютными значениями коэффициентов после стандартизации признаков.\n",
        "\n",
        "Random Forest: использует MDI (Mean Decrease in Impurity) — измеряет, насколько каждый признак уменьшает неопределенность при разбиениях по всем деревьям. В вашей ЛР sleep_study_inter доминирует с важностью ~0.35.\n",
        "\n",
        "XGBoost: несколько типов важности — weight (частота использования признака), gain (среднее улучшение качества), cover (относительное количество наблюдений). Gain часто наиболее информативен.\n",
        "\n",
        "Интерпретация: важность показывает относительный вклад, но не причинно-следственную связь. Коррелирующие признаки могут «делить» важность между собой.\n",
        "\n",
        "15. Почему XGBoost часто работает лучше, чем Random Forest, на структурированных данных?\n",
        "\n",
        "Градиентный бустинг vs Бэггинг: XGBoost последовательно исправляет ошибки, каждое дерево фокусируется на сложных случаях. Random Forest усредняет независимые предсказания, что может «размывать» сигнал.\n",
        "\n",
        "Оптимизация: XGBoost использует продвинутые техники оптимизации — второй порядок градиентов (Newton-Raphson), встроенная регуляризация, обработка пропущенных значений. Random Forest полагается на простое усреднение.\n",
        "\n",
        "Гибкость: XGBoost предоставляет множество гиперпараметров для тонкой настройки — различные функции потерь, схемы регуляризации, стратегии выборки. Обработка дисбаланса: лучше справляется с несбалансированными данными через веса классов.\n",
        "\n",
        "16. Что такое переобучение и как его можно выявить на графике обучения?\n",
        "\n",
        "Переобучение (overfitting) — модель «запоминает» обучающие данные вместо изучения общих закономерностей. Отлично работает на train set, плохо на новых данных.\n",
        "\n",
        "Признаки на learning curves: train score продолжает расти, validation score стагнирует или падает — классический признак переобучения. Большой разрыв между train и validation метриками. Validation loss растет после определенной точки, несмотря на снижение train loss.\n",
        "\n",
        "В нашей ЛР: хорошие результаты (R²=0.891) без большого разрыва между train/test указывают на отсутствие серьезного переобучения благодаря правильной регуляризации и feature engineering.\n",
        "\n",
        "Методы борьбы: регуляризация (Ridge/Lasso), early stopping, dropout, кросс-валидация, увеличение данных, уменьшение сложности модели.\n",
        "\n",
        "17. Как работает метод ансамблирования в Random Forest (bagging)?\n",
        "\n",
        "Bagging (Bootstrap Aggregating) — каждое дерево обучается на случайной подвыборке с возвращением (bootstrap sample) размером равным исходной выборке. Некоторые объекты попадают несколько раз, другие не попадают вовсе (~37% остаются out-of-bag).\n",
        "\n",
        "Дополнительная случайность: в каждом узле каждого дерева рассматривается случайное подмножество признаков (обычно √p для классификации, p/3 для регрессии). Это снижает корреляцию между деревьями.\n",
        "\n",
        "Агрегация результатов: для регрессии — среднее арифметическое предсказаний всех деревьев, для классификации — мажоритарное голосование. В вашей ЛР 200 деревьев усредняют свои предсказания экзаменационных баллов.\n",
        "\n",
        "Преимущества bagging: снижение дисперсии модели, защита от переобучения, параллелизуемость обучения, out-of-bag оценка качества без отдельной валидационной выборки.\n",
        "\n",
        "18. В чем отличие бустинга от бэггинга?\n",
        "\n",
        "Бэггинг (Random Forest): деревья обучаются параллельно и независимо на разных подвыборках. Каждое дерево имеет равный вес в финальном решении. Цель — снизить дисперсию через усреднение.\n",
        "\n",
        "Бустинг (XGBoost): деревья обучаются последовательно, каждое следующее исправляет ошибки предыдущих. Более поздние деревья фокусируются на сложных случаях. Цель — снизить bias через итеративное улучшение.\n",
        "\n",
        "Веса объектов: в бэггинге все объекты равноважны, в бустинге объекты с большими ошибками получают больший вес в следующей итерации.\n",
        "\n",
        "Риски: бэггинг редко переобучается благодаря усреднению, бустинг склонен к переобучению при избыточных итерациях.\n",
        "\n",
        "Performance: бустинг часто достигает лучшего качества на train set, но требует более аккуратной настройки для хорошей генерализации.\n",
        "\n",
        "19. Какие способы feature engineering можно применить к вашему датасету?\n",
        "Взаимодействие признаков: sleep_hours × study_hours, attendance × study_hours, mental_health × sleep_hours — выявляют синергетические эффекты.\n",
        "\n",
        "Полиномиальные признаки: study_hours², sleep_hours² — моделируют нелинейности (например, оптимальное количество сна). Пропорции: leisure_time/study_time, social_media/total_screen_time — относительные метрики важнее абсолютных.\n",
        "\n",
        "Временные паттерны: если есть временные данные — дни недели, сезонность, тренды. Биннинг: группировка непрерывных признаков в категории — \"мало сна\" (< 6h), \"норма\" (6-8h), \"много\" (> 8h).\n",
        "\n",
        "Доменные знания: создание композитных индексов — \"здоровый образ жизни\" = exercise + sleep + diet, \"академическая активность\" = study_hours + attendance.\n",
        "\n",
        "Кодирование категорий: target encoding для высококардинальных категорий, frequency encoding, label encoding с учетом порядка.\n",
        "\n",
        "20. Как использовать кросс-валидацию для подбора гиперпараметров моделей регрессии?\n",
        "\n",
        "K-Fold Cross-Validation: данные разделяются на K фолдов (обычно 5 или 10), модель обучается на K-1 фолдах, тестируется на оставшемся. Процедура повторяется K раз, результаты усредняются.\n",
        "\n",
        "GridSearchCV: перебор всех комбинаций гиперпараметров из заданной сетки. Для каждой комбинации выполняется кросс-валидация. Выбирается комбинация с лучшей средней метрикой.\n",
        "\n",
        "RandomizedSearchCV: случайная выборка из пространства гиперпараметров. Эффективнее GridSearch при большом количестве параметров.\n",
        "\n",
        "Стратегии для регрессии:\n",
        "\n",
        "Linear models: подбор alpha для регуляризации (10^-4 до 10^2)\n",
        "\n",
        "Random Forest: n_estimators (100-1000), max_depth (None, 10, 20), min_samples_split (2-10)\n",
        "\n",
        "XGBoost: learning_rate (0.01-0.3), max_depth (3-10), n_estimators (100-2000)\n",
        "\n",
        "Метрики для оценки: R², RMSE, MAE в зависимости от задачи. Nested CV: внешняя кросс-валидация для оценки качества, внутренняя — для подбора гиперпараметров, избегает переобучения на валидации."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
